{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation of numbers\n",
    "#### Integers\n",
    " - Base b system: $(base=b\\in\\mathbb{Z}^+): \\forall x\\in\\mathbb{Z}^+. x={(d_nd_{n-1}...d_0)}_b=\\sum_{i=0}^n d_i\\times b^i, 0\\leq d_i < b, d_i\\in \\mathbb{Z}^+$\n",
    " - Adding an extra sign to represent positive / negative, in computer, 0/1\n",
    " - Converting decimal to base b: repeatedly divide the decimal number x by the base b, take the remainder each time, then the remainders are $d_0,...,d_n$\n",
    "\n",
    "#### Reals\n",
    " - Base b system: extend the integers representation to the fraction part: $.x={(.d_{-1}d_{-2}...d_{-n})}_b=\\sum_{i=1}^\\infty d_{-i}\\times b^{-i}$\n",
    " - Every binary fraction can be mapped to a decimal fraction, but not the converse\n",
    " - Converting decimal to binary: repeatedly multiple the decimal fraction x by 2, take the integer, until the fraction part is 0. Then the integers are $d_{-1},...d_{-n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Representation of Numbers\n",
    "#### Simplified form\n",
    "$$x = (f)_b\\times b^{(e)_b} \\\\ \n",
    "f = \\pm(.d_1d_2...d_t)_b\\:(mantissa \\:|\\: significand) \\\\\n",
    "e = \\pm (c_{s-1}c_{s-2}...c_0)_b\\:(exponent\\:|\\: characteristic)$$\n",
    "- **normalized floating-point number**  if $d_1\\neq 0$ or $f=0$\n",
    "- $0\\leq|f|<1$\n",
    "\n",
    "#### IEEE Standard form\n",
    "The IEEE Standard form is the way most computers representing binary numbers, which is  \n",
    "$$(-1)^q \\times (d_0.d_1d_2...d_{t-1})_b\\times 2^e \\\\\n",
    "E_{min} \\leq e \\leq E_{max} (E_{min} = - E_{max} + 1), e = (-1)^p \\times (c_{s-2}...c_1c_0)_b \\\\\n",
    "q, p \\in \\{0,1\\}, d_i, c_i \\in \\{0,1\\}\\\\\n",
    "$$\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td>Type of number</td>\n",
    "<td>number of bits</td>\n",
    "<td>$E_{min}$</td>\n",
    "<td>$E_{max}$</td>\n",
    "<td>t</td>\n",
    "<td>$\\epsilon_{mach}$</td>\n",
    "</tr></thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>single precision, binary32</td>\n",
    "<td>32</td>\n",
    "<td>-126</td>\n",
    "<td>127</td>\n",
    "<td>23+1 = 24</td>\n",
    "<td>$1.2\\times 10^{-7}$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>double precision, binary64</td>\n",
    "<td>64</td>\n",
    "<td>-1022</td>\n",
    "<td>1023</td>\n",
    "<td>52 + 1 = 53</td>\n",
    "<td>$2.2\\times 10^{-16}$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>quadruple precision, binary128</td>\n",
    "<td>128</td>\n",
    "<td>-16382</td>\n",
    "<td>16383</td>\n",
    "<td>112+1=113</td>\n",
    "<td>$1.9\\times 10^{-34}$</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    " \n",
    " - The mantissa is normalized, $d_0$ is always saved to be 1/ \n",
    " - IEEE uses proper rounding\n",
    " - includes \"special\" numbers (symbols) for indeterminate values (e.x. $\\infty, -\\infty, NaN$)\n",
    " \n",
    "\n",
    "#### Limits\n",
    "- The exponent $e$ is limited by $E_{min}\\leq e\\leq E_{max}, -E_{min}=E_{max}=(aa...a)_b, a = b-1$\n",
    "- The largest floating point number **Overflow Level (OFL)** is $N_{max} = (.aa...a)_b\\times b^{(aa...a)_b}$\n",
    "- The smallest floating point number **Underflow Level (UFL)** is $N_{min}=(.100...0)_b\\times b^{-(aa...a)_b}$ (normalized, what we often consider), $N_{min}=(.00...1)_b\\times b^{-(aa...a)_b}$ (non-normalized)\n",
    "\n",
    "#### Set of floating point numbers\n",
    "- **$R_b(t,s)$** the set of all base b floating point numbers that can be represented by $t$ b-digits mantissa and $s$ b-digits exponent\n",
    "- $R_b(t,s) = [-OFL, -UFL]\\cup {0} \\cup [UFL, OFL]$, **Overflow** if in $(-\\infty, -OFL)\\cup(OFL,\\infty)$, **Underflow** if in $(-UFL,UFL)-{0}$\n",
    "- $R_b(t,s)$ is finite, discrete, and more condensed towards 0\n",
    "\n",
    "#### Rounding\n",
    "$\\forall x \\in \\mathbb{R}, fl(x)\\in\\mathbb{R}_b(t,s)$ is the common way to convert a real to a floating point number. The common ways are\n",
    " - Chopping: simply flooring after $t$th digit of the mantissa\n",
    " - Traditional rounding: round up if $D_{i+1}\\geq b/2$, down if $D_{i+1} < b/2$\n",
    " - Proper (perfect) rounding: round up if $D_{i+1}\\ge b/2$, down if $D_{i+1} < b/2$, round to the nearest even if $D_{i+1} = b/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation Error\n",
    "**round-off error**: $fl(x) - x$\n",
    " - **relative round-off error** $\\delta = \\frac{fl(x)-x}{x}, |\\delta|\\leq b^{1-t} (normalized, chopping), |\\delta|\\leq b^{1-t}/2 (normalized, rounding)$\n",
    " - **Correct in $r$ significant $b$-digits** if the approximation $\\hat{x}$ to $x$ is $|\\frac{x-\\hat{x}}{x}|=|\\delta| \\leq \\frac{b^{1-r}}{2}$.<br>\n",
    "\n",
    "**Error in arithmetic** \n",
    " - Let $\\circ$ be an arithmetic operator (or any function $f:\\mathbb{R}_b(t,s)\\rightarrow \\mathbb{R}_b(t,s)$, e.x. $+,-,\\times, /,sin$), then the computer's floating-point operation $\\bar{\\circ}$ is constructed so that $x\\bar{\\circ} y = fl(x\\circ y)$\n",
    "\n",
    "**Machine epsilon $(\\epsilon_{mach})$**\n",
    " - The smallest (non-normalized) floating-point number such that $1+\\epsilon_{mach} > 1$, $\\epsilon_{mach} = \\delta$ bounds, a.k.a $|\\delta| \\leq \\epsilon_{mach}$, note $b^{1-t}/2 < \\epsilon_{mach} = b^{1-t}/2 + b^{-t} \\leq b^{1-t}$ if proper rounding.\n",
    " \n",
    " **Error propagation**\n",
    " As soon as an error rises, it may then be amplified or reduced in subsequent operations. <br>\n",
    " Let $x,y\\in\\mathbb{R}, fl(x) = x(1+\\delta_x), fl(y)=y(1+\\delta_y)$\n",
    " - Multiplication\n",
    "$$fl(x)\\bar{\\times}fl(y) = fl(fl(x)\\times fl(y)) \\\\\n",
    "  = (x(1+\\delta_x)y(1+\\delta_y))(1+\\delta_{xy}) \\\\ \n",
    "  = xy(1+\\delta_x + \\delta_y + \\delta_{xy} + \\delta_x \\delta_y + \\delta_x \\delta_{xy} + \\delta_{xy}\\delta_y + \\delta_x\\delta_y\\delta_{xy}) \\\\\n",
    "  \\approx xy(1+\\delta_x+\\delta_y+\\delta_{xy}) \\quad\\text{the rest are their product, hence much smaller} \\\\\n",
    "  |\\delta_\\times| \\leq 3 \\epsilon_{mach}\n",
    "$$\n",
    " - Addition\n",
    " $$\n",
    "     fl(x)\\bar{+}fl(y) = fl(fl(x)+fl(y)) \\\\ \n",
    "     = (x(1+\\delta_x) + y(1+\\delta_y))(1+\\delta_{x+y}) \\\\ \n",
    "     = x(1+\\delta_x)(1+\\delta_{x+y}) + y(1+\\delta_y)(1+\\delta_{x+y}) \\\\\n",
    "     \\approx x(1+\\delta_x+\\delta_{x+y}) + y(1+\\delta_y+\\delta_{x+y}) \\\\\n",
    "     = (x+y)(1 + \\frac{x(\\delta_x+\\delta_{x+y})}{x+y} + \\frac{y(\\delta_y+\\delta_{x+y})}{x+y}) \\\\\n",
    "     |\\delta_+| \\leq |x/x+y|2\\epsilon_{mach}+|y/x+y|2\\epsilon_{mach} = \\frac{|x|+|y|}{|x+y|}2\\epsilon_{mach} \\\\\n",
    "     = 2\\epsilon_{mach} \\:|\\: xy>0 \\\\ \n",
    "     = 2\\epsilon_{mach}\\frac{|x-y|}{|x+y|}\\:|\\: xy<0\n",
    " $$\n",
    " Consider $x\\rightarrow -y$, the relative error is extreme high\n",
    " - The possible error occurred in addition (adding nearly opposite numbers) is called **catastrophic cancellation**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
