{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation of numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integers\n",
    " - Base b system: $(base=b\\in\\mathbb{Z}^+): \\forall x\\in\\mathbb{Z}^+. x={(d_nd_{n-1}...d_0)}_b=\\sum_{i=0}^n d_i\\times b^i, 0\\leq d_i < b, d_i\\in \\mathbb{Z}^+$\n",
    " - Adding an extra sign to represent positive / negative, in computer, 0/1\n",
    " - Converting decimal to base b: repeatedly divide the decimal number x by the base b, take the remainder each time, then the remainders are $d_0,...,d_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reals\n",
    " - Base b system: extend the integers representation to the fraction part: $.x={(.d_{-1}d_{-2}...d_{-n})}_b=\\sum_{i=1}^\\infty d_{-i}\\times b^{-i}$\n",
    " - Every binary fraction can be mapped to a decimal fraction, but not the converse\n",
    " - Converting decimal to binary: repeatedly multiple the decimal fraction x by 2, take the integer, until the fraction part is 0. Then the integers are $d_{-1},...d_{-n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Representation of Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified form\n",
    "$$x = (f)_b\\times b^{(e)_b} \\\\ \n",
    "f = \\pm(.d_1d_2...d_t)_b\\:(mantissa \\:|\\: significand) \\\\\n",
    "e = \\pm (c_{s-1}c_{s-2}...c_0)_b\\:(exponent\\:|\\: characteristic)$$\n",
    "- **normalized floating-point number**  if $d_1\\neq 0$ or $f=0$\n",
    "- $0\\leq|f|<1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IEEE Standard form\n",
    "The IEEE Standard form is the way most computers representing binary numbers, which is  \n",
    "$$(-1)^q \\times (d_0.d_1d_2...d_{t-1})_b\\times 2^e \\\\\n",
    "E_{min} \\leq e \\leq E_{max} (E_{min} = - E_{max} + 1), e = (-1)^p \\times (c_{s-2}...c_1c_0)_b \\\\\n",
    "q, p \\in \\{0,1\\}, d_i, c_i \\in \\{0,1\\}\\\\\n",
    "$$\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td>Type of number</td>\n",
    "<td>number of bits</td>\n",
    "<td>$E_{min}$</td>\n",
    "<td>$E_{max}$</td>\n",
    "<td>t</td>\n",
    "<td>$\\epsilon_{mach}$</td>\n",
    "</tr></thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>single precision, binary32</td>\n",
    "<td>32</td>\n",
    "<td>-126</td>\n",
    "<td>127</td>\n",
    "<td>23+1 = 24</td>\n",
    "<td>$1.2\\times 10^{-7}$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>double precision, binary64</td>\n",
    "<td>64</td>\n",
    "<td>-1022</td>\n",
    "<td>1023</td>\n",
    "<td>52 + 1 = 53</td>\n",
    "<td>$2.2\\times 10^{-16}$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>quadruple precision, binary128</td>\n",
    "<td>128</td>\n",
    "<td>-16382</td>\n",
    "<td>16383</td>\n",
    "<td>112+1=113</td>\n",
    "<td>$1.9\\times 10^{-34}$</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    " \n",
    " - The mantissa is normalized, $d_0$ is always saved to be 1/ \n",
    " - IEEE uses proper rounding\n",
    " - includes \"special\" numbers (symbols) for indeterminate values (e.x. $\\infty, -\\infty, NaN$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limits\n",
    "- The exponent $e$ is limited by $E_{min}\\leq e\\leq E_{max}, -E_{min}=E_{max}=(aa...a)_b, a = b-1$\n",
    "- The largest floating point number **Overflow Level (OFL)** is $N_{max} = (.aa...a)_b\\times b^{(aa...a)_b}$\n",
    "- The smallest floating point number **Underflow Level (UFL)** is $N_{min}=(.100...0)_b\\times b^{-(aa...a)_b}$ (normalized, what we often consider), $N_{min}=(.00...1)_b\\times b^{-(aa...a)_b}$ (non-normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set of floating point numbers\n",
    "- **$R_b(t,s)$** the set of all base b floating point numbers that can be represented by $t$ b-digits mantissa and $s$ b-digits exponent\n",
    "- $R_b(t,s) = [-OFL, -UFL]\\cup {0} \\cup [UFL, OFL]$, **Overflow** if in $(-\\infty, -OFL)\\cup(OFL,\\infty)$, **Underflow** if in $(-UFL,UFL)-{0}$\n",
    "- $R_b(t,s)$ is finite, discrete, and more condensed towards 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rounding\n",
    "$\\forall x \\in \\mathbb{R}, fl(x)\\in\\mathbb{R}_b(t,s)$ is the common way to convert a real to a floating point number. The common ways are\n",
    " - Chopping: simply flooring after $t$th digit of the mantissa\n",
    " - Traditional rounding: round up if $D_{i+1}\\geq b/2$, down if $D_{i+1} < b/2$\n",
    " - Proper (perfect) rounding: round up if $D_{i+1}\\ge b/2$, down if $D_{i+1} < b/2$, round to the nearest even if $D_{i+1} = b/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### round-off error $fl(x) - x$\n",
    " - **relative round-off error** $\\delta = \\frac{fl(x)-x}{x}, |\\delta|\\leq b^{1-t} (normalized, chopping), |\\delta|\\leq b^{1-t}/2 (normalized, rounding)$\n",
    " - **Correct in $r$ significant $b$-digits** if the approximation $\\hat{x}$ to $x$ is $|\\frac{x-\\hat{x}}{x}|=|\\delta| \\leq \\frac{b^{1-r}}{2}$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error in arithmetic\n",
    " - Let $\\circ$ be an arithmetic operator (or any function $f:\\mathbb{R}_b(t,s)\\rightarrow \\mathbb{R}_b(t,s)$, e.x. $+,-,\\times, /,sin$), then the computer's floating-point operation $\\bar{\\circ}$ is constructed so that $x\\bar{\\circ} y = fl(x\\circ y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine epsilon $(\\epsilon_{mach})$\n",
    " - The smallest (non-normalized) floating-point number such that $1+\\epsilon_{mach} > 1$, $\\epsilon_{mach} = \\delta$ bounds, a.k.a $|\\delta| \\leq \\epsilon_{mach}$, note $b^{1-t}/2 < \\epsilon_{mach} = b^{1-t}/2 + b^{-t} \\leq b^{1-t}$ if proper rounding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error propagation**\n",
    " As soon as an error rises, it may then be amplified or reduced in subsequent operations. <br>\n",
    " Let $x,y\\in\\mathbb{R}, fl(x) = x(1+\\delta_x), fl(y)=y(1+\\delta_y)$\n",
    " - Multiplication\n",
    "$$fl(x)\\bar{\\times}fl(y) = fl(fl(x)\\times fl(y)) \\\\\n",
    "  = (x(1+\\delta_x)y(1+\\delta_y))(1+\\delta_{xy}) \\\\ \n",
    "  = xy(1+\\delta_x + \\delta_y + \\delta_{xy} + \\delta_x \\delta_y + \\delta_x \\delta_{xy} + \\delta_{xy}\\delta_y + \\delta_x\\delta_y\\delta_{xy}) \\\\\n",
    "  \\approx xy(1+\\delta_x+\\delta_y+\\delta_{xy}) \\quad\\text{the rest are their product, hence much smaller} \\\\\n",
    "  |\\delta_\\times| \\leq 3 \\epsilon_{mach}\n",
    "$$\n",
    " - Addition\n",
    " $$\n",
    "     fl(x)\\bar{+}fl(y) = fl(fl(x)+fl(y)) \\\\ \n",
    "     = (x(1+\\delta_x) + y(1+\\delta_y))(1+\\delta_{x+y}) \\\\ \n",
    "     = x(1+\\delta_x)(1+\\delta_{x+y}) + y(1+\\delta_y)(1+\\delta_{x+y}) \\\\\n",
    "     \\approx x(1+\\delta_x+\\delta_{x+y}) + y(1+\\delta_y+\\delta_{x+y}) \\\\\n",
    "     = (x+y)(1 + \\frac{x(\\delta_x+\\delta_{x+y})}{x+y} + \\frac{y(\\delta_y+\\delta_{x+y})}{x+y}) \\\\\n",
    "     |\\delta_+| \\leq |\\frac{x}{x+y}|2\\epsilon_{mach}+|\\frac{y}{x+y}|2\\epsilon_{mach} = \\frac{|x|+|y|}{|x+y|}2\\epsilon_{mach} \\\\\n",
    "     = 2\\epsilon_{mach} \\:|\\: xy>0 \\\\ \n",
    "     = 2\\epsilon_{mach}\\frac{|x-y|}{|x+y|}\\:|\\: xy<0\n",
    " $$\n",
    "Consider $x\\rightarrow -y$, the relative error is extreme high\n",
    " - The possible error occurred in addition (adding nearly opposite numbers) is called **catastrophic cancellation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Propagation\n",
    "Consider some $x\\in\\mathbb{R}$ and output $f(x)\\in\\mathbb{R}$. <br>\n",
    "Let $fl(x)=x(1+\\delta_x)$, assume $f(x)$ is twice differentiable near $x$. Then, if $f(x)\\neq 0$, Taylor's series give $f(fl(x))=f(x)(1+\\delta_f) + o(\\delta_x^2)$ ignore the higher order terms, $\\delta_f = \\frac{xf'(x)}{f(x)}\\delta_x, |\\delta_f|\\leq |\\frac{xf'(x)}{f(x)}|\\epsilon_{mach}$ <br>\n",
    "**(relative) condition number** $k_f=|\\frac{xf'(x)}{f(x)}|$ measures the relative sensitivity of the computation of $f(x)$ on relatively small changes in the input $x$ (how the relative error in $x$ propagates in $f(x)$)\n",
    " - **Well-conditioned** if relatively small changes in the input produces relatively small changes in the output, otherwise **ill-conditioned**\n",
    " - Examples \n",
    "   - $f(x):=\\sqrt{x}, k_f = |\\frac{xf'(x)}{f(x)}| = 1/2$ is well-conditioned.\n",
    "   - $f(x)=e^x, k_f = |\\frac{xf'(x)}{f(x)}| = |x|$ since for large input, the function itself will overflow faster, it is well-conditioned.\n",
    " - Note if some functions have $k_f > \\epsilon_{mach}^{-1}$, we risk having no correct digits at all, $k_f$ is a inherent property to the function, not to the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stability of algorithms \n",
    " A numerical algorithm is **stable** if small changes in the algorithm input have a small effect on the output, otherwise **unstable**. \n",
    " - Example: consider $(15.6-15.7)^2$ in 3 decimal digits with rounding, \n",
    " $$(15.6-15.7)^2 = (-.1)^2 = .1\\times 10^{-1} \\\\\n",
    " 15.6^2 - 2(15.6)(15.7) + 15.7^2 = 243 - 490 + 246 = -.1\\times 10^1$$ <br>\n",
    " Mathematically equivalent expressions are not necessarily computationally equivalent.\n",
    " - Efforts taken to get more stable algorithm\n",
    "  - avoid adding nearly opposite numbers\n",
    "  - minimize the number of operations\n",
    "  - When adding several numbers, to add them starting from the smallest and proceeding to the largest\n",
    "  - to be alert when adding numbers of very different scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward and backward errors\n",
    "Consider $y = f(x)$, assume $f^{-1}$ exists, assume instead of $y$, we compute $\\hat{y}$ due to various errors. <br>\n",
    "**Forward error** $y=\\hat{y}$ includes initial data error.<br>\n",
    "Let $\\hat{x} = f^{-1}(\\hat{y})$, then $x-\\hat{x}$ is the **backward error** <br>\n",
    "We can also consider $\\hat{y}$ as being the result of inexact computation $\\hat{f}$, $\\hat{y}=\\hat{f}(x)$, therefore $\\hat{f}(x) = f(\\hat{x}), \\hat{x}=f^{-1}(\\hat{f}(x))$\n",
    "- Relative forward error = $\\frac{y-\\hat{y}}{y}$, relative back error = $\\frac{x-\\hat{x}}{x}$, and $|RFE|\\approx k_f |RBE|$\n",
    "- Example $x = 2, f(x):=\\sqrt{x}, \\hat{y}= 1.4$. \n",
    "   - $\\hat{x} = f^{-1}(1.4) = 1.96$\n",
    "   - Forward error $y - \\hat{y} = 1.4142... - 1.4 = 0.0142...$\n",
    "   - Relative forward error $\\frac{y-\\hat{y}}{y} = \\frac{0.0142...}{1,4142...}\\approx 0.01$\n",
    "   - Backward error = $x-\\hat{x} = 2 - 1.96 = 0.04$\n",
    "   - Relative backward error = $\\frac{x-\\hat{x}}{x} = 0.04 / 2 = 0.02$\n",
    "   - Condition number = 1/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncation and rounding errors\n",
    "**Truncation** Errors happens when mathematical expressions are approximated by other expressions, assuming the computations in the approximate expressions are performed in exact arithmetic. <br> \n",
    "The evaluation of the approximate expressions may be performed in finite arithmetic, it may involve additional error as **rounding error**<br>\n",
    "**Computational error** is the sum of the two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Error\n",
    "Consider the input $x$, its computer representation $\\hat{x}$, the target computation $f(x)$, and its approximation function $g(x)$, the computer evaluates $\\hat{g}(x)$, then **total error** $$f(x)-\\hat{g}(\\hat{x}) = (f(x)-f(\\hat{x})) + [(f(\\hat{x})-g(\\hat{x})) + (g(\\hat{x}) - \\hat{g}(\\hat{x}))] \\\\\n",
    "= (propagated) + [(truncation) + (rounding)] $$\n",
    " - Example: $\\sin(\\pi/8)$, let $x=\\pi, \\hat{x}=3, f(x)=sin(x), g(x)=x-x^3/3!$, the computation be done in 3-decimal digit floating point arithmetic. Then $\\hat{g}(\\hat{x}) = fl(fl(3/8) - fl(fl(1/3!)fl(fl(3/8)^3))) \\approx 0.366$\n",
    "  - initial data error $x-\\hat{x} = \\pi - 3 \\approx 0.14158...$ \n",
    "  - propagated error $f(x) - f(\\hat{x}) = \\sin(\\pi/8) - \\sin(8/3)\\approx 0.0164$\n",
    "  - truncation error $f(\\hat{x}) - g(\\hat{x}) = \\sin(3/8) - (3/8 - (3/8)^3/6 \\approx 0.0000616$\n",
    "  - rounding error $g(\\hat{x}) - \\hat{g}(\\hat{x}) = 3/8 - (3/8)^3/6 - fl(3/8 - (3/8)^3/6) = 0.0002109$\n",
    "  - Total error $f(x) - \\hat{g}(\\hat{x}) = 0.38268343... - 0.366 = 0.01668343 \\approx 0.0164 + 0.0000616 + 0.0002109$\n",
    " - General conclusions\n",
    "   - mathematical operations are not always equivalent to the respective computational operations \n",
    "   - Not all mathematical formulas and other properties hold in computer arithmetic\n",
    "   - in most cases, small errors arise, which we should be alerted for some cases\n",
    "   - using maximum available precision is suggested\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taylor's Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $k\\in\\mathbb{Z}^+, a\\in\\mathbb{R}, x\\in\\mathbb{R}$. Assume $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is $k+1$ differentiable on $(a, x)$ and continuous on $[a, x]$, then $\\exists\\epsilon$ s.t. $f(x) = (\\sum_{i=0}^k \\frac{f^{(i)}(a)}{i!}(x-a)^i) + \\frac{f^{(k+1)}(\\epsilon)}{(k+1)!} (x-a)^{k+1}$. <br>\n",
    "Then, let $t_k(x) = \\sum_{i=0}^k \\frac{f^{(i)}(a)}{i!}(x-a)^i$ is a good approximation. <br>\n",
    "$R_{k+1}(x) = \\frac{f^{(k+1)}(\\epsilon)}{(k+1)!} (x-a)^{k+1}$ is the truncation error. <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
